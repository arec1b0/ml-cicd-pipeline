# API and Component Reference

Comprehensive reference for the public surface area exposed by this project, including HTTP endpoints, factory functions, reusable components, and training utilities. Paths in headings point to the defining module.

## FastAPI Application (`src/app/main.py`)

- `create_app() -> fastapi.FastAPI`
  - Configures logging using `LOG_LEVEL`.
  - Registers `health`, `predict`, and `metrics` routers.
  - Attaches `PrometheusMiddleware` for telemetry.
  - Startup hook loads a serialized model from `MODEL_PATH`, optional `metrics.json`, and sets `app.state` attributes (`ml_wrapper`, `ml_metrics`, `is_ready`). When accuracy is available it updates the `MODEL_ACCURACY` gauge.
  - Shutdown hook clears in-memory state and resets accuracy gauge to 0.
- `app`: Module-level instance returned by `create_app()` for ASGI servers (`uvicorn src.app.main:app`).

### Environment Configuration (`src/app/config.py`)

- `get_env(name: str, default: str | None = None) -> str | None`: Reads environment variables with optional fallback.
- `MODEL_PATH`: Defaults to `/app/model_registry/model.pkl`; change via `MODEL_PATH` env variable.
- `LOG_LEVEL`: Defaults to `INFO`; accepts any standard `logging` level name.

**Example: start the service with custom paths**

```bash
MODEL_PATH=/workspace/model_registry/model.pkl LOG_LEVEL=debug uvicorn src.app.main:app --host 0.0.0.0 --port 8000
```

## HTTP API Surface (`src/app/api`)

### `GET /health/`

- Declared in `health.health` with response model `HealthResponse` (`status: str`, `ready: bool`, `details: dict | None`).
- Always returns `status="ok"`; `ready` reflects whether startup successfully loaded a model.
- `details.metrics` echoes contents of `metrics.json` when available.

**Example request**

```bash
curl -s http://localhost:8000/health/ | jq .
# {
#   "status": "ok",
#   "ready": true,
#   "details": {"metrics": {"accuracy": 0.9733}}
# }
```

### `POST /predict/`

- Declared in `predict.predict` with request model `PredictRequest` (`features: List[List[float]]`) and response model `PredictResponse` (`predictions: List[int]`).
- Validates that each feature row is a non-empty list of numbers. Returns HTTP 400 with a descriptive message on shape errors.
- Responds with HTTP 503 if the model is not loaded (`app.state.is_ready` is false) and HTTP 500 if model inference raises.
- Converts integer-like floats to Python `int` for compact JSON payloads.

**Example request**

```bash
curl -s -X POST http://localhost:8000/predict/ \
  -H "Content-Type: application/json" \
  -d '{"features": [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.7, 1.5]]}'
# {"predictions":[0,1]}
```

**Handling errors programmatically**

```python
import httpx

resp = httpx.post("http://localhost:8000/predict/", json={"features": [[]]})
if resp.status_code == 400:
    raise ValueError(resp.json()["detail"])  # "features[0] must be a non-empty list of numbers"
```

### `GET /metrics/`

- Declared in `metrics.metrics`; returns the Prometheus exposition format generated by `metrics_response()`.
- `include_in_schema=False` keeps it out of the autogenerated OpenAPI docs.

**Example request**

```bash
curl -s http://localhost:8000/metrics/ | head
# HELP ml_request_count Total HTTP requests processed
# TYPE ml_request_count counter
# ml_request_count{method="GET",path="/health/",status="200"} 42.0
# ...
```

## Telemetry Utilities (`src/utils/telemetry.py`)

- `PrometheusMiddleware`: Starlette middleware that wraps each request, measuring latency and counting results. Metrics collected:
  - `ml_request_count{method, path, status}` (`Counter`)
  - `ml_request_latency_seconds{method, path}` (`Histogram`)
  - `ml_request_errors_total{method, path}` (`Counter` for uncaught exceptions and 5xx responses)
  - `ml_model_accuracy` (`Gauge` updated after startup)
- `metrics_response() -> starlette.responses.Response`: Serializes the global Prometheus registry.

**Reusing the middleware in another FastAPI app**

```python
from fastapi import FastAPI
from src.utils.telemetry import PrometheusMiddleware, metrics_response

api = FastAPI()
api.add_middleware(PrometheusMiddleware)

@api.get("/metrics", include_in_schema=False)
async def metrics():
    return metrics_response()
```

## Inference Helpers (`src/models/infer.py`)

- `ModelWrapper`
  - Constructor accepts any object exposing `predict(np.ndarray) -> array-like` (e.g., scikit-learn models).
  - `predict(features: Sequence[Sequence[float]]) -> list`: Converts inputs to a NumPy array, calls the wrapped model, and returns a Python list.
- `load_model(path: pathlib.Path) -> ModelWrapper`: Deserializes a Joblib artifact and wraps it.

**Batch prediction example**

```python
from pathlib import Path
from src.models.infer import load_model

wrapper = load_model(Path("model_registry/model.pkl"))
preds = wrapper.predict([[5.1, 3.5, 1.4, 0.2]])
print(preds)  # [0]
```

## Training Workflow (`src/models/trainer.py`)

- `TrainResult`: Dataclass containing `model_path: Path` and `accuracy: float`.
- `train(output_path: Path) -> TrainResult`
  - Loads the Iris dataset, trains a `RandomForestClassifier`, and evaluates on a held-out validation set.
  - Persists the trained estimator to `output_path` via Joblib.
  - Writes `metrics.json` next to the model with `{"accuracy": <float>}`.
  - Returns accuracy for downstream checks.
- CLI usage: `python -m src.models.trainer --output model_registry/model.pkl --metrics model_registry/metrics.json`

**Programmatic training and service startup**

```python
from pathlib import Path
from src.models import trainer
from src.app.main import create_app

result = trainer.train(Path("model_registry/model.pkl"))
print(f"Trained model with accuracy {result.accuracy:.3f}")

app = create_app()  # now picks up the freshly trained model on startup
```

## Error Handling & Readiness

- During startup, failure to load the model logs an exception and keeps `app.state.is_ready = False`. Predict calls will emit HTTP 503 until resolved.
- `metrics.json` is optional; missing or malformed files are ignored gracefully.

## Testing Hooks

- Unit tests (`tests/unit`) demonstrate training and inference usage patterns; mirror these when writing new integrations.
- For data pipeline smoke checks, ensure `data/processed/train.csv` and `data/processed/val.csv` exist so that `tests/test_data_pipeline.py` passes.

