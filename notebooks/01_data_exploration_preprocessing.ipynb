{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Data Exploration & Preprocessing\n",
        "Purpose: reproducible, Windows-friendly data ingestion and validation pipeline.\n",
        "Principles: single responsibility functions, clear typing, deterministic outputs.\n",
        "\n",
        "Usage:\n",
        " - Run cell-by-cell.\n",
        " - Outputs: `data/raw/iris.csv`, `data/processed/train.csv`, `data/processed/val.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "High-level imports and environment setup.\n",
        "\"\"\"\n",
        "\n",
        "# Standard library\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Third-party\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 — deterministic sample dataset (Iris)\n",
        "We use Iris as a reproducible example dataset. In production, replace loader with GCS/S3 connector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a reproducible example dataset (Iris).\n",
        "    Returns:\n",
        "        pd.DataFrame: features + target\n",
        "    \"\"\"\n",
        "    from sklearn.datasets import load_iris\n",
        "    iris = load_iris(as_frame=True)\n",
        "    df = pd.concat([iris.frame, iris.target.rename(\"target\")], axis=1)\n",
        "    logging.info(\"Loaded iris dataset with shape %s\", df.shape)\n",
        "    return df\n",
        "\n",
        "df = load_sample_dataset()\n",
        "df.to_csv(RAW_DIR / \"iris.csv\", index=False)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 — Schema and validation utilities\n",
        "Minimal deterministic schema check suitable for CI unit tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA = {\n",
        "    \"feature_cols\": [\"sepal length (cm)\",\"sepal width (cm)\",\"petal length (cm)\",\"petal width (cm)\"],\n",
        "    \"target_col\": \"target\",\n",
        "    \"n_features\": 4,\n",
        "}\n",
        "\n",
        "def validate_schema(df: pd.DataFrame, schema: Dict) -> None:\n",
        "    \"\"\"Raise ValueError on schema mismatch.\"\"\"\n",
        "    missing = [c for c in schema[\"feature_cols\"] + [schema[\"target_col\"]] if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns: {missing}\")\n",
        "    if df[schema[\"feature_cols\"]].isna().any().any():\n",
        "        raise ValueError(\"NaNs present in feature columns\")\n",
        "    logging.info(\"Schema validation passed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate_schema(df, SCHEMA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 — Deterministic preprocessing pipeline\n",
        "Single function that performs transformations and returns train/val splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_and_split(df: pd.DataFrame, test_size: float=0.2, random_seed: int=42) -> Tuple[pd.DataFrame,pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Deterministically preprocesses the DataFrame and returns train and validation splits.\n",
        "    - Simple standardization using train statistics (no data leakage).\n",
        "    \"\"\"\n",
        "    feature_cols = SCHEMA[\"feature_cols\"]\n",
        "    target = SCHEMA[\"target_col\"]\n",
        "\n",
        "    X = df[feature_cols].astype(float)\n",
        "    y = df[target].astype(int)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_seed, stratify=y)\n",
        "\n",
        "    # Standardize (fit on train only)\n",
        "    mu = X_train.mean(axis=0)\n",
        "    sigma = X_train.std(axis=0).replace(0,1.0)\n",
        "\n",
        "    X_train_scaled = (X_train - mu) / sigma\n",
        "    X_val_scaled = (X_val - mu) / sigma\n",
        "\n",
        "    train = pd.concat([X_train_scaled.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
        "    val = pd.concat([X_val_scaled.reset_index(drop=True), y_val.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Persist standardization metadata\n",
        "    (PROCESSED_DIR / \"scaler.json\").write_text(json.dumps({\"mu\": mu.to_dict(), \"sigma\": sigma.to_dict()}))\n",
        "    train.to_csv(PROCESSED_DIR / \"train.csv\", index=False)\n",
        "    val.to_csv(PROCESSED_DIR / \"val.csv\", index=False)\n",
        "\n",
        "    logging.info(\"Preprocessing complete. Train / Val shapes: %s / %s\", train.shape, val.shape)\n",
        "    return train, val\n",
        "\n",
        "train_df, val_df = preprocess_and_split(df)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 — Quick EDA checks (these are lightweight and work in CI)\n",
        "We avoid heavy plotting here; CI-friendly assertions follow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_data_checks(train: pd.DataFrame, val: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Basic assertions to fail fast in CI:\n",
        "    - No NaNs\n",
        "    - Correct number of features\n",
        "    - Class balance check (not too imbalanced)\n",
        "    \"\"\"\n",
        "    assert not train.isna().any().any(), \"NaNs in train\"\n",
        "    assert not val.isna().any().any(), \"NaNs in val\"\n",
        "    assert train.shape[1] == SCHEMA[\"n_features\"] + 1, \"Unexpected number of columns in train\"\n",
        "    # class balance check: not required but informative\n",
        "    class_counts = train[SCHEMA[\"target_col\"]].value_counts(normalize=True).to_dict()\n",
        "    logging.info(\"Train class distribution: %s\", class_counts)\n",
        "\n",
        "basic_data_checks(train_df, val_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "End of notebook 01.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
